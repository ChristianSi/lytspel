#!/usr/bin/python3
"""Convert a file or a text block from tradspell to Lytspel.

Copyright (c) 2018-2019 Christian Siefkes

Permission to use, copy, modify, and/or distribute this software for any
purpose with or without fee is hereby granted, provided that the above
copyright notice and this permission notice appear in all copies.

THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH
REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT,
INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM
LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR
OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR
PERFORMANCE OF THIS SOFTWARE.
"""


import argparse
from collections import Counter
import csv
from enum import Enum
from inspect import trace
from io import BytesIO
from os import path
import re
from sys import argv, exc_info, stderr
from typing import Dict, List, Sequence, TextIO, TypeVar, Union
import warnings
from warnings import warn
from zipfile import is_zipfile, ZipFile

from lxml import etree, html
import spacy


# Constants and type variables
SCRIPTNAME = 'lytspelconv'

LOWERCASE_LETTER_PAT = '[a-záàăâǎåäãąāảæćĉčċçďđéèĕêěëẽęēẻǵğĝǧġḡĥȟḧħíìĭîǐïĩįīĵǰḱǩĺľłḹḿńǹňñóòŏôǒöõøǫōỏœṕŕřśŝšşßťẗúùŭûǔůüũųūủṽẃẁẘẅẍýỳŷẙÿỹȳỷźẑž]'
TOKEN_RE = re.compile('(' + LOWERCASE_LETTER_PAT + "(?:['’]?" + LOWERCASE_LETTER_PAT +')*)',
        re.IGNORECASE)

XHTML_NAMESPACE = '{http://www.w3.org/1999/xhtml}'

BLOCK_LEVEL_TAGS = set(('address', 'article', 'aside', 'blockquote', 'canvas', 'dd', 'div',
        'dl', 'dt', 'fieldset', 'figcaption', 'figure', 'footer', 'form', 'h1', 'h2',
        'h3', 'h4', 'h5', 'h6', 'header', 'hr', 'li', 'main', 'nav', 'noscript', 'ol',
        'output', 'p', 'pre', 'section', 'table', 'tfoot', 'ul', 'video'))

POS_FALLBACKS = {'aj': 'n', 'av': 'aj', 'n': 'aj', 'prp': 'aj', 'v': 'n'}

T = TypeVar('T')

ConvState = Enum('ConvState', 'LOOKS_FOREIGN NLP_NEEDED')


# Initialization code

if __name__ == '__main__':
    def compact_warning(message, category, filename, lineno, line=None):
        """Print warnings without showing the warning message line itself."""
        fname = path.split(filename)[1]
        return '{}:{}: {}: {}\n'.format(fname, lineno, category.__name__, message)

    warnings.formatwarning = compact_warning


def dict_filename() -> str:
    """Return the name of the dictionary file.

    The file is supposed to sit in a '../data' directory relative to the location of the
    current script.
    """
    script_dir = path.dirname(__file__)
    joined_path = path.join(script_dir, '../data/lytspel-dict.csv')
    return path.abspath(joined_path)  # eliminate '../' part


def is_mixed_case(word: str) -> bool:
    """Test whether a word is MiXed case.

    A word is assumed to be MiXed case if it starts with an upper-case letter and if it
    contains at least one other upper-case and one lower-case letter. Words must be
    ASCII-fied for this function to work correctly.
    """
    return re.match('[A-Z].*[a-z]', word) and re.search('.[A-Z]', word)


def get_elem(seq: Sequence[T], idx: int) -> T:
    """Savely retried an element from a sequence.

    None is returned if 'seq' ends before the requested 'idx' position.
    """
    if len(seq) > idx:
        return seq[idx]
    else:
        return None


def load_dict() -> Dict[str, str]:
    """Load the dictionary.

    Also loads the 'mixed_dict' dictionary which must already exist as an (empty) global.
    """
    with open(dict_filename(), encoding='utf-8') as csvfile:
        csvreader = csv.reader(csvfile)
        next(csvreader)  # skip header line
        result = {}
        redirects = {}  # will be resolved later

        for row in csvreader:
            tradspell = get_elem(row, 0)
            pos = get_elem(row, 1)
            redirect = get_elem(row, 2)
            lytspel = get_elem(row, 3)

            if tradspell and lytspel:
                ts_lower = tradspell.lower()
                ls_lower = lytspel.lower()

                if pos:
                    # Treat value as a dict of POS-tagged entries
                    if not ts_lower in result:
                        result[ts_lower] = {}
                    result[ts_lower][pos] = ls_lower
                else:
                    result[ts_lower] = ls_lower

                if is_mixed_case(lytspel):
                    mixed_dict[tradspell] = lytspel

            elif tradspell and redirect:
                redirects[tradspell.lower()] = redirect.lower()
            else:
                warn('Unexpected/malformed CSV row: {}'.format(','.join(row)))

    # Resolve redirects
    for key, target in redirects.items():
        value = result.get(target)
        if value:
            result[key] = value
        else:
            warn("Target '{}' of redirect '{}' missing!".format(target, key))

    return result


# Globals
mixed_dict = {}
ls_dict = load_dict()
# Load spaCy, disabling unnecessary components
nlp = spacy.load('en', disable=['parser', 'ner'])

# Global switch, consulted by lookup() and updated during conversion and tokenization
at_sent_start = True

# Optional counter of unknown words
unknownCounter = None


# Main code

def asciify(word: str) -> str:
    """Convert a word to its ASCII equivalent.

    Typographic apostrophes are replaced by normal ones and diacritical letters are
    replaced by their closest ASCII equivalents.
    """
    result = word.replace('’', "'")  # ASCII-ify apostrophes

    if not re.match("[-'a-zA-Z]*$", result): # non-ASCII word
        letters = list(result)

        for i, letter in enumerate(letters):
            if ord(letter) < 128: continue  # ASCII, nothing to do
            lower_letter = letter.lower()
            replacement = ''

            # vowels and semivowels
            if lower_letter in 'áàăâǎåäãąāả':
                replacement = 'a'
            elif lower_letter in 'æ':
                replacement = 'ae'
            elif lower_letter in 'éèĕêěëẽęēẻ':
                replacement = 'e'
            elif lower_letter in 'íìĭîǐïĩįī':
                replacement = 'i'
            elif lower_letter in 'óòŏôǒöõøǫōỏ':
                replacement = 'o'
            elif lower_letter in 'œ':
                replacement = 'oe'
            elif lower_letter in 'úùŭûǔůüũųūủ':
                replacement = 'u'
            elif lower_letter in 'ýỳŷẙÿỹȳỷ':
                replacement = 'y'
            # consonants
            elif lower_letter in 'ćĉčċç':
                replacement = 'c'
            elif lower_letter in 'ďđ':
                replacement = 'd'
            elif lower_letter in 'ǵğĝǧġḡ':
                replacement = 'g'
            elif lower_letter in 'ĥȟḧħ':
                replacement = 'h'
            elif lower_letter in 'ĵǰ':
                replacement = 'j'
            elif lower_letter in 'ḱǩ':
                replacement = 'k'
            elif lower_letter in 'ĺľłḹ':
                replacement = 'l'
            elif lower_letter in 'ḿ':
                replacement = 'm'
            elif lower_letter in 'ńǹňñ':
                replacement = 'n'
            elif lower_letter in 'ṕ':
                replacement = 'p'
            elif lower_letter in 'ŕř':
                replacement = 'r'
            elif lower_letter in 'śŝšş':
                replacement = 's'
            elif lower_letter in 'ß':
                replacement = 'ss'
            elif lower_letter in 'ťẗ':
                replacement = 't'
            elif lower_letter in 'ṽ':
                replacement = 'v'
            elif lower_letter in 'ẃẁẘẅ':
                replacement = 'w'
            elif lower_letter in 'ẍ':
                replacement = 'x'
            elif lower_letter in 'źẑž':
                replacement = 'z'

            if replacement:
                if letter != lower_letter:  #  original was upper case
                    replacement = replacement.upper()
                letters[i] = replacement

        result = ''.join(letters)
    return result


def translate_pos(spacy_pos: str) -> str:
    """Translate a POS tag as used by spaCy into the form used in the dictionary."""
    if spacy_pos in ('AUX', 'VERB'):
        return 'v'
    elif spacy_pos in ('NOUN', 'PROPN'):
        return 'n'
    elif spacy_pos == 'ADJ':
        return 'aj'
    elif spacy_pos == 'ADV':
        return 'av'
    elif spacy_pos == 'ADP':
        return 'prp'
    else:
        # Other POS tags shouldn't usually occur regarding our POS-tagged words
        warn('Unexpected spaCy POS tag: {}'.format(spacy_pos))
        return 'v'


def find_pos_tagged_entry(entries: dict, pos: str, tried_variants: List[str] = None) -> str:
    """Find the entry to use if several POS-tagged spellings exist for a word.

    'pos' is the POS tag as returned by spaCy (capitalized) or used in our dictionary
    (lower-case) - the former will be translated into the later if needed.

    'tried_variants' is used internally to prevent endless loops in case another POS tag
    has to be tried as fallback.
    """
    if pos and pos[0].isupper():
        pos = translate_pos(pos)
    if tried_variants is None:
        tried_variants = []

    tried_variants.append(pos)
    result = entries.get(pos)

    if result is None:
        fallback = POS_FALLBACKS.get(pos)

        if fallback is None or fallback in tried_variants:
            if fallback is None:
                warn("No fallback found for POS tag '{}'; using last entry of {} as fallback"
                        .format(pos, entries))
            else:
                warn("Trying POS tag '{}' after {} would loop; using last entry of {} as fallback"
                        .format(pos, tried_variants, entries))

            result = entries[sorted(entries.keys())[-1]]
        else:
            result = find_pos_tagged_entry(entries, fallback, tried_variants)

    return result


def lookup(word: str, spacy_pos: str = None) -> str:
    """Lookup a word in the dictionary and returned its converted form.

    'spacy_pos' is the POS tag as returned by spaCy. If it's omitted and if the respelling
    of the word depends on its POS tag, ConvState.NLP_NEEDED is returend.

    None is returned if the word is unknown.

    Case is restored (lower, Capitalized, or ALL_CAPS). MixedCase is also restored
    provided that *both* the input word and the dictionary entry use this case form
    (e.g. JavaScript -> JaavaScript).
    """
    word = asciify(word)
    # Strip final 's (genitive or contraction) and remember for later (handling case)
    word, genitiveS = stripGenitiveS(word)

    if word == 'US':
        result = word
    else:
        lower = word.lower()

        if lower in ("'ll", "'re", "'ve"):
            # Contraction tokens produced by spaCy: discard the last letter
            result = lower[:-1]
        elif re.match("'.$", lower):
            # Single-letter contraction token: return as is
            result = lower
        else:
            result = ls_dict.get(lower)
            if result is None:
                return result

            if isinstance(result, dict):
                if spacy_pos:
                    result = find_pos_tagged_entry(result, spacy_pos)
                else:
                    return ConvState.NLP_NEEDED

        if re.match("[-'A-Z]+$", word):
            result = result.upper()  # ALL_CAPS
        elif re.match('[A-Z]', word):
            if word in mixed_dict:
                result = mixed_dict[word]  #  MixedCase
            else:
                result = result[0].upper() + result[1:]  # Capitalized

        if not at_sent_start and re.match("I$|I['’]", word):
            # Correct the case of 'I' and its contractions
            result = result.lower()

    if genitiveS:
        result += genitiveS

    result = result.replace( "'", '’')  # replace normal by typographic apostrophe
    return result


def tokenize_text(text: str) -> List[str]:
    """Tokenize a string, returning an array of words and puncuation.
    Words must start and end with a letter and may contain apostrophes.
    """
    if not text:
        return []  # Empty or None

    result = TOKEN_RE.split(text)
    # Remove first and/or last element if they are empty
    if result[0] == '':
        result.pop(0)
    if result[-1] == '':
        result.pop()
    return result


def is_word(token: str) -> bool:
    """Check if a token is a word.

    Words start with a letter, or with an apostrophe followed by a letter."""
    return re.match("['’]?" + LOWERCASE_LETTER_PAT, token, re.IGNORECASE)




def text_looks_foreign(text: str) -> bool:
    """Check whether a text fragment looks like it's written in a foreign language.

    Returns True iff a majority of the words are unknown.
    """
    in_tokens = tokenize_text(text)
    known_words = 0
    unknown_words = 0

    for token in in_tokens:
        if is_word(token):
            conv = lookup(token)
            if conv:
                known_words += 1
            else:
                unknown_words += 1

    return unknown_words > known_words


def ends_sentence(token: str) -> bool:
    """Uses some simple heuristics to determine whether a token seems to end a sentence.

    Returns True in one of three cases:

    * The tokens ends in a dot, question or exclamation mark, optionally followed by an
      opening or closing quote marks
    * It ends in a colon followed by an opening quote mark
    * It ends in '>' (quoted text marker), optionally followed by an opening quote mark

    Whitespace is ignored in all cases.
    """
    if not token:
        return False
    if re.search('([.?!](\s*["\'“‘”’])?|:\s*["\'“‘]|>(\s*["\'“‘])?)\s*$', token):
        return True
    else:
        return False


def stripGenitiveS(word: str):
    """Strip the genitive or contraction "'s" from a word, if its present.

    Returns two values: actual word, genitive. If the word doesn't end in a genitive, the
    first return value will be identical to the argument and the second return value will
    be empty.

    Both simple and typographic apostrophes are recognized.
    """
    if re.search(".'s$", word, re.IGNORECASE):
        return word[:-2], word[-2:]
    else:
        return word, ''


def convert_text_if_simple(text: str, test_if_foreign: bool = True,
        starts_sent: bool = True) -> object:
    """Convert a text fragment if doing so is possible without POS tagging.

    'starts_sent' is an optional attribute that specifies whether this text fragment is likely
    to start with a new sentence. If true, the 'at_sent_start' switch is set accordingly,
    otherwise it is left alone.

    The return value is either:

    * The converted text (a string)
    * ConvState.NLP_NEEDED if POS tagging (NLP) is needed)
    * ConvState.LOOKS_FOREIGN if 'test_if_foreign' is true and a majority of the words in
      the fragment are unknown
    """
    global at_sent_start, unknownCounter
    orig_at_sent_start = at_sent_start  # Remember in case we have to restore it later

    if not text:
        return text  # Empty or None, nothing to do
    if starts_sent:
        at_sent_start = True

    in_tokens = tokenize_text(text)
    out_tokens = []
    lasttok = ''
    known_words = 0
    unknown_words = 0

    if unknownCounter is not None:
        localUnknownCounter = Counter()

    for token in in_tokens:
        if is_word(token):
            conv = lookup(token)

            if conv is ConvState.NLP_NEEDED:
                at_sent_start = orig_at_sent_start
                return ConvState.NLP_NEEDED
            elif conv:
                known_words += 1
                out_tokens.append(conv)
            else:
                unknown_words += 1
                out_tokens.append(token)

                if unknownCounter is not None:
                    actualWord = stripGenitiveS(token)[0]
                    localUnknownCounter[actualWord] += 1

            at_sent_start = False
        else:
            # Not a word
            if token == '-' and lasttok:
                # Check if this forms a hyphenated prefix with the preceding token, e.g. 're-'
                conv = lookup(lasttok + token)

                if conv:
                    out_tokens[-1] = conv
                else:
                    out_tokens.append(token)
            else:
                # Append non-word as is and check whether it terminates a sentence
                out_tokens.append(token)
                at_sent_start |= ends_sentence(token)

        lasttok = token

    if not test_if_foreign or unknown_words <= known_words:
        # We shouldn't make the foreign language test OR at least half of the words are known
        if unknownCounter is not None:
            unknownCounter += localUnknownCounter

        return ''.join(out_tokens)
    else:
        return ConvState.LOOKS_FOREIGN


def convert_para(text: str, test_if_foreign: bool = True, starts_sent: bool = True) -> str:
    """Convert a paragraph.

    'starts_sent' is an optional attribute that specifies whether this text fragment is likely
    to start with a new sentence. If true, the 'at_sent_start' switch is set accordingly,
    otherwise it is left alone.

    If 'test_if_foreign' is True and a majority of the words in the paragraph are unknown,
    the paragraph is assumed to be written in a foreign language and returned unchanged.
    """
    global at_sent_start

    if not text:
        if starts_sent:
            # Nothing to do, but we still update the global state (a new block-level HTML
            # element might have been opened)
            at_sent_start = True
        return text

    simple_result = convert_text_if_simple(text, test_if_foreign, starts_sent)

    if simple_result is ConvState.NLP_NEEDED:
        # We have to invoke spaCy for POS tagging (unless the text looks foreign)
        if text_looks_foreign(text):
            return text  # Return text unchanged

        doc = nlp(text)
        tokens = []
        lasttok = ''
        last_nonword = ''

        if starts_sent:
            at_sent_start = True

        for entry in doc:
            token = entry.text
            tail = entry.whitespace_ or ''  # optional trailing whitespace

            if token.lower() in ("n't", 'n’t'):
                # SpaCy treats "n't" (as in "don't" etc.) as a separate word, but we look it up
                # together with the preceding word because the joined pronounciation (and hence
                # spelling) is sometimes different
                if tokens:
                    tokens.pop()
                token = lasttok + token

            if is_word(token):
                if token.endswith('’'):
                    # SpaCy gets confused by typographic half quotation marks,
                    # so we strip them ourselves
                    tail = token[-1:] + tail
                    token = token[:-1]

                conv = lookup(token, entry.pos_)

                if conv:
                    tokens.append(conv)
                else:
                    tokens.append(token)

                at_sent_start = ends_sentence(tail)
                last_nonword = ''
            else:
                # Not a word
                if token == '-' and not tail and lasttok:
                    # Check if this forms a hyphenated prefix with the preceding token, e.g. 're-'
                    conv = lookup(lasttok + token)

                    if conv:
                        tokens[-1] = conv
                    else:
                        tokens.append(token)
                        last_nonword = token
                else:
                    # Append as is
                    tokens.append(token)

                    # We also consider the last nonword token for cases such as ‹: "› (colon
                    # followed by quote) which are considered two tokens by spaCy
                    at_sent_start |= ends_sentence(last_nonword + token + tail)
                    last_nonword = token

            lasttok = token

            # Append trailing whitespace to token, if any
            if tail:
                tokens[-1] += tail

        return ''.join(tokens)
    elif simple_result is ConvState.LOOKS_FOREIGN:
        return text  # Return text unchanged
    else:
        return simple_result


def determine_file_type(filename: str) -> str:
    """Inspect the contents of a file to determine the likely file type.

    Returns either 'epub', 'html', 'txt', or None if the file is clearly not of any of these
    types. However, 'txt' is used as a fairly general fallback hence it's quite possible that
    a file labeled as 'txt' is actually something else
    """
    # Check if it's an epub file
    if is_zipfile(filename):
        with ZipFile(filename) as zip:
            bytes = b''

            try:
                bytes = zip.read('mimetype')
            except KeyError:
                pass  # Not an epub file

            if bytes.decode().startswith('application/epub+zip'):
                return 'epub'
            else:
                return None

    with open(filename) as file:
        for line in file:
            line = line.strip()
            if not line: continue  # Empty line, inspect next one
            if re.match("<[!?h]", line, re.IGNORECASE):
                # File seems to start with a DOCTYPE or XML declaration, HTML comment or
                # <html> tag
                return 'html'
            else:
                break

    return 'txt'


def simple_tag(elem: etree._Element) -> str:
    """Return the tag name of an Element without the XHTML namespace, if used.

    If the element lives within the XHTML namespace, just the local name is returned,
    e.g. '{http://www.w3.org/1999/xhtml}img' becomes 'img'.

    In all other cases, the tag name is returned unchanged.
    """
    tag = elem.tag

    if tag is None:
        return tag

    if tag.startswith(XHTML_NAMESPACE):
        return tag[len(XHTML_NAMESPACE):]
    else:
        return tag


def convert_html_elem(elem: etree._Element) -> None:
    """Recursively convert an element in an HTML document and its children.

    Whether to convert textual content is decided on the level of block-level tags (such
    as 'h1', 'blockquote', 'p') that do NOT contain any directly nested block-level tags
    (e.g. if an 'ol' contains 'li' elements, the decision will be made for each of the
    latter independently, not for the whole 'ol'). If a large part of the text embedded
    in such an element seems to be in a foreign language, the whole element will NOT be
    converted.
    """
    # Check if we should made the foreign-language test as this level
    if simple_tag(elem) in BLOCK_LEVEL_TAGS:
        decide_on_conversion = True

        for child in elem:
            if simple_tag(child) in BLOCK_LEVEL_TAGS:
                decide_on_conversion = False
                break

        if decide_on_conversion:
            full_text = str(etree.XPath("string()")(elem))
            if text_looks_foreign(full_text):
                # Skip this part of the document tree (doesn't seem to be English)
                return

    # Convert immediate content (only block-level elements count as start of a new paragraph
    elem.text = convert_para(elem.text, False, starts_sent=simple_tag(elem) in BLOCK_LEVEL_TAGS)

    # Convert child elements (except comments and those that don't contain normal text)
    for child in elem:
        if not (isinstance(child, etree._Comment)
                or isinstance(child, etree._ProcessingInstruction)
                or simple_tag(child) in ('script', 'style')):
            convert_html_elem(child)

        if child.tail:
            child.tail = convert_para(child.tail, False, starts_sent=False)

    # Convert a few textual attributes, if they are present
    for attrib in ('alt', 'title'):
        if attrib in elem.attrib and elem.attrib[attrib]:
            elem.attrib[attrib] = convert_para(elem.attrib[attrib])


def convert_html_document(filename_or_bytes: Union[str, BytesIO]) -> bytes:
    """Convert and return an HTML or XHTML file.

    Returns an UTF-8 encoded bytestring.
    """
    doc = None
    is_xhtml = True

    # Try parsing as XHTML, if that doesn't work, parse as regular HTML
    try:
        doc = etree.parse(filename_or_bytes)
    except etree.XMLSyntaxError:
        doc = html.parse(filename_or_bytes)
        is_xhtml = False

    root = doc.getroot()
    title = root.find('.//title', namespaces=root.nsmap)
    body = root.find('body', namespaces=root.nsmap)

    if title is not None:
        title.text = convert_para(title.text)
    if body is not None:
        convert_html_elem(body)

    if is_xhtml:
        return etree.tostring(doc, encoding='utf8')
    else:
        return html.tostring(doc, encoding='utf8')


def convert_xml_elem(elem: etree._Element) -> None:
    """Recursively convert an element in an XML document and its children.

    This function is only meant for elements that aren't (X)HTML. All textual content of
    the element and its child elements are converted (unless they look foreign), while
    all attribute values are left alone. In HTML, on the other hand, certain elements
    (such as 'script') are skipped and certain attributes (such as 'alt') are converted.

    Also, while in HTML the foreign-language test is made at the level of block-level tags
    (such as 'p' or 'li'), here every text fragment is tested independently.
    """
    # Convert immediate content
    elem.text = convert_para(elem.text)

    # Convert child elements (except comments and PIs)
    for child in elem:
        if not (isinstance(child, etree._Comment)
                or isinstance(child, etree._ProcessingInstruction)):
            convert_xml_elem(child)
        child.tail = convert_para(child.tail, starts_sent=False)


def convert_xml_document(filename_or_bytes: Union[str, BytesIO]) -> bytes:
    """Convert and return an XML file.

    This function is only meant for documents that aren't (X)HTML.

    Returns an UTF-8 encoded bytestring.
    """
    doc = etree.parse(filename_or_bytes)
    convert_xml_elem(doc.getroot())
    return etree.tostring(doc, encoding='utf8')


def make_hrefs_absolute(items: Sequence[etree._Element], dirname: str) -> Sequence[etree._Element]:
    """Convert a list of XML elements with filenames from relative into absolute filenames.

    Each member of the 'items' sequence must have a 'href' attribute that will be
    modified accordingly.

    If dirname is empty, the original list is returned unchanged.
    """
    if dirname:
        for item in items:
            item.attrib['href'] = path.join(dirname, item.attrib['href'])

    return items


def convert_epub(filename: str) -> None:
    """Convert an epub file.

    If the input file is called 'FILE.epub', the output will be stored in a new file called
    'FILE-lytspel.epub'.
    """
    (root, ext) = path.splitext(filename)
    outfile = '{}-lytspel{}'.format(root, ext)

    with ZipFile(filename, 'r') as zin:
        # Find the contents metafile
        bytes = zin.read('META-INF/container.xml')
        tree = etree.fromstring(bytes)
        # Usually there is just one OPF file, but multiple-rendition epubs have several ones
        opf_files = tree.xpath('n:rootfiles/n:rootfile/@full-path',
                namespaces={'n': 'urn:oasis:names:tc:opendocument:xmlns:container'})
        absolute_items = []

        # Find the XML files that need conversion
        for opf_file in opf_files:
            bytes = zin.read(opf_file)
            tree = etree.fromstring(bytes)
            relative_items = tree.xpath('/p:package/p:manifest/p:item',
                    namespaces={'p': 'http://www.idpf.org/2007/opf'})
            absolute_items += make_hrefs_absolute(relative_items, path.dirname(opf_file))

        html_files = set()
        ncx_files = set()

        for item in absolute_items:
            media_type = item.attrib['media-type']

            if media_type == 'application/xhtml+xml':
                html_files.add(item.attrib['href'])
            elif media_type == 'application/x-dtbncx+xml':
                # Deprecated, but might occur in older epubs
                ncx_files.add(item.attrib['href'])

        # Copy files to output archive, converting them if needed
        with ZipFile(outfile, 'w') as zout:
            zout.comment = zin.comment  # Preserve the comment, if any

            for item in zin.infolist():
                bytes = zin.read(item.filename)

                if item.filename in html_files:
                    bio = BytesIO(bytes)
                    bytes = convert_html_document(bio)
                if item.filename in ncx_files or item.filename in opf_files:
                    bio = BytesIO(bytes)
                    bytes = convert_xml_document(bio)

                zout.writestr(item, bytes)

    print('{}: Output written to {}'.format(SCRIPTNAME, outfile), file=stderr)


def convert_text_document(filename: str) -> None:
    """Convert a plain text file."""
    para = ''

    with open(filename) as file:
        for line in file:
            line = line.rstrip()
            # Paragraphs are considered to be separated by empty lines.
            # However, very long lines (200+ chars) are considered paragraphs in their own right.
            if len(line) >= 200:  # stand-alone paragraph
                if para:
                    print(convert_para(para))
                    para = ''
                print(convert_para(line))
            elif line:            # regular line
                if para:
                    para += '\n'
                para += line
            else:                 # empty line
                if para:
                    print(convert_para(para))
                    para = ''
                print()

        # Convert final paragraph, if any
        if para:
            print(convert_para(para))


def convert_file(filename: str) -> None:
    """Convert the file with the specified name.

    Recognized file types are epub, HTML and plain text.
    """
    file_type = determine_file_type(file)

    if path.getsize(filename) / 1024 >= 256:
        print('{}: Converting {} -- this may take a while...'.format(SCRIPTNAME, filename),
                file=stderr)

    if file_type == 'epub':
        convert_epub(file)
    elif file_type == 'html':
        bytes = convert_html_document(file)
        print(bytes.decode('utf8'))
    elif file_type == 'txt':
        convert_text_document(file)
    elif file_type is None:
        exit('{}: Cannot convert {} (unsupported file type)'.format(SCRIPTNAME, filename))
    else:
        raise ValueError('Unexpected file type: {}'.format(file_type))


if __name__ == '__main__':
    try:
        parser = argparse.ArgumentParser()
        parser.add_argument('files', metavar='FILE', nargs='*',
                help='files to convert')
        parser.add_argument('-u', '--unknown', action='count', default=0,
                help='list unknown words (repeat option n times to list only words that occur'
                    ' at least that often)')
        args = parser.parse_args()

        if args.unknown:
            unknownCounter = Counter()
            print('***Counting unknown words***')

        for file in args.files:
            convert_file(file)

        if args.unknown:
            # TODO Print unknown words
            print('Unknown words: {}'.format(unknownCounter))
    except Exception as e:
        tb = exc_info()[2]
        # Find highest stack frame in the current file (closest to source of exception)
        frames = trace()
        last_useful_frame = frames[0]
        current_file = last_useful_frame.filename

        for frame in frames[1:]:
            if frame.filename == current_file:
                last_useful_frame = frame
            else:
                break  # Stepping out of current file

        fname = path.split(current_file)[1]
        exit('{}:{}: {}: {}'.format(fname, last_useful_frame.lineno, e.__class__.__name__, e))


# TODO Handle and test:
# * Allow converting text passed on command line (-c flag)
# * Epub conversion: ensure that zip compression is as it should be
# * Use pylint to check the coding style
# * -o flag to specify an output file (requires a single input file)
# * Add button to turn conversion off and on
# * Accept '-' argument to read stdin (file type recognition should still work)
# * If called w/o arguments open dialog that allows the conversion of local txt/html/epub files,
#   always writing the output to a file
# * Add a dialog for converting words and sentences to Lytspel
# * HTML conversion: check whether complex mixed text handled correctly (foreign language testing)
# * Package for pip3 and announce to the world
# * Add drag-and-drop support (esp. for Windows)
# * Profile epub conversion and remove any obvious bottlenecks
