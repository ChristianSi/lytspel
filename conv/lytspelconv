#!/usr/bin/python3
"""Convert a file or a text block from tradspell to Lytspel.

Copyright (c) 2018-2019 Christian Siefkes

Permission to use, copy, modify, and/or distribute this software for any
purpose with or without fee is hereby granted, provided that the above
copyright notice and this permission notice appear in all copies.

THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH
REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT,
INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM
LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR
OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR
PERFORMANCE OF THIS SOFTWARE.
"""


import argparse
from collections import Counter
import csv
from enum import Enum
from inspect import trace
from io import BytesIO
from os import path
import re
from sys import argv, exc_info, stderr
from typing import Dict, List, Sequence, TextIO, TypeVar, Union
import warnings
from warnings import warn
from zipfile import is_zipfile, ZipFile

from lxml import etree, html
import spacy


# Constants and type variables
SCRIPTNAME = 'lytspelconv'

LOWERCASE_LETTER_PAT = '[a-záàăâǎåäãąāảæćĉčċçďđéèĕêěëẽęēẻǵğĝǧġḡĥȟḧħíìĭîǐïĩįīĵǰḱǩĺľłḹḿńǹňñóòŏôǒöõøǫōỏœṕŕřśŝšşßťẗúùŭûǔůüũųūủṽẃẁẘẅẍýỳŷẙÿỹȳỷźẑž]'
TOKEN_RE = re.compile('(' + LOWERCASE_LETTER_PAT + "(?:['’]?" + LOWERCASE_LETTER_PAT +')*)',
        re.IGNORECASE)

XHTML_NAMESPACE = '{http://www.w3.org/1999/xhtml}'

BLOCK_LEVEL_TAGS = set(('address', 'article', 'aside', 'blockquote', 'canvas', 'dd', 'div',
        'dl', 'dt', 'fieldset', 'figcaption', 'figure', 'footer', 'form', 'h1', 'h2',
        'h3', 'h4', 'h5', 'h6', 'header', 'hr', 'li', 'main', 'nav', 'noscript', 'ol',
        'output', 'p', 'pre', 'section', 'table', 'tfoot', 'ul', 'video'))

POS_FALLBACKS = {'aj': 'n', 'av': 'aj', 'n': 'aj', 'prp': 'aj', 'v': 'n'}

T = TypeVar('T')

ConvState = Enum('ConvState', 'LOOKS_FOREIGN NLP_NEEDED')


# Initialization code

if __name__ == '__main__':
    def compact_warning(message, category, filename, lineno, line=None):
        """Print warnings without showing the warning message line itself."""
        fname = path.split(filename)[1]
        return '{}:{}: {}: {}\n'.format(fname, lineno, category.__name__, message)

    warnings.formatwarning = compact_warning


# Utility functions

def get_elem(seq: Sequence[T], idx: int) -> T:
    """Savely retried an element from a sequence.

    None is returned if 'seq' ends before the requested 'idx' position.
    """
    if len(seq) > idx:
        return seq[idx]
    else:
        return None


# Classes

class Dictionary:
    """A dictionary mapping tradspell to Lytspel.

    Instances of this class are threadsafe after initialization. Usually it should be
    enough to have a single instance per program.
    """

    def __init__(self):
        """Initializes and loads the dictionary."""
        self._dict = {}
        self._mixed_dict = {}

        with open(self._dict_filename(), encoding='utf-8') as csvfile:
            csvreader = csv.reader(csvfile)
            next(csvreader)  # skip header line
            redirects = {}  # will be resolved later

            for row in csvreader:
                tradspell = get_elem(row, 0)
                pos = get_elem(row, 1)
                redirect = get_elem(row, 2)
                lytspel = get_elem(row, 3)

                if tradspell and lytspel:
                    ts_lower = tradspell.lower()
                    ls_lower = lytspel.lower()

                    if pos:
                        # Treat value as a dict of POS-tagged entries
                        if not ts_lower in self._dict:
                            self._dict[ts_lower] = {}
                        self._dict[ts_lower][pos] = ls_lower
                    else:
                        self._dict[ts_lower] = ls_lower

                    if self.is_mixed_case(lytspel):
                        self._mixed_dict[tradspell] = lytspel

                elif tradspell and redirect:
                    redirects[tradspell.lower()] = redirect.lower()
                else:
                    warn('Unexpected/malformed CSV row: {}'.format(','.join(row)))

        # Resolve redirects
        for key, target in redirects.items():
            value = self._dict.get(target)
            if value:
                self._dict[key] = value
            else:
                warn("Target '{}' of redirect '{}' missing!".format(target, key))

    def _dict_filename(self) -> str:
        """Return the name of the dictionary file.

        The file is supposed to sit in a '../data' directory relative to the location of the
        current script.
        """
        script_dir = path.dirname(__file__)
        joined_path = path.join(script_dir, '../data/lytspel-dict.csv')
        return path.abspath(joined_path)  # eliminate '../' part

    def is_mixed_case(self, word: str) -> bool:
        """Test whether a word is MiXed case.

        A word is assumed to be MiXed case if it starts with an upper-case letter and if it
        contains at least one other upper-case and one lower-case letter. Words must be
        ASCII-fied for this function to work correctly.
        """
        return re.match('[A-Z].*[a-z]', word) and re.search('.[A-Z]', word)

    def lookup(self, word: str, spacy_pos: str = None, at_sent_start: bool = False) -> Union[
            str, ConvState]:
        """Lookup a word in the dictionary and returned its converted form.

        'spacy_pos' is the POS tag as returned by spaCy. If it's omitted and if the respelling
        of the word depends on its POS tag, ConvState.NLP_NEEDED is returend.

        'at_sent_start' should be set to true if this token is the first of a new sentence.
        It is used to handle the case of the converted pronoun 'I' and its contractions.

        None is returned if the word is unknown.

        Case is restored (lower, Capitalized, or ALL_CAPS). MixedCase is also restored
        provided that *both* the input word and the dictionary entry use this case form
        (e.g. JavaScript -> JaavaScript).
        """
        word = self.asciify(word)
        # Strip final 's (genitive or contraction) and remember for later (handling case)
        word, genitiveS = self.strip_genitive_s(word)

        if word in ('BIOS', 'US'):
            # Keep some capitalized abbreviations unchanged
            result = word
        else:
            lower = word.lower()

            if lower in ("'ll", "'re", "'ve"):
                # Contraction tokens produced by spaCy: discard the last letter
                result = lower[:-1]
            elif re.match("'.$", lower):
                # Single-letter contraction token: return as is
                result = lower
            else:
                result = self._dict.get(lower)
                if result is None:
                    return result

                if isinstance(result, dict):
                    if spacy_pos:
                        result = self._find_pos_tagged_entry(result, spacy_pos)
                    else:
                        return ConvState.NLP_NEEDED

            if re.match("[-'A-Z]+$", word):
                result = result.upper()  # ALL_CAPS
            elif re.match('[A-Z]', word):
                if word in self._mixed_dict:
                    result = self._mixed_dict[word]  #  MixedCase
                else:
                    result = result[0].upper() + result[1:]  # Capitalized

            if not at_sent_start and re.match("I$|I['’]", word):
                # Correct the case of 'I' and its contractions
                result = result.lower()

        if genitiveS:
            result += genitiveS

        result = result.replace( "'", '’')  # replace normal by typographic apostrophe
        return result

    def asciify(self, word: str) -> str:
        """Convert a word to its ASCII equivalent.

        Typographic apostrophes are replaced by normal ones and diacritical letters are
        replaced by their closest ASCII equivalents.
        """
        result = word.replace('’', "'")  # ASCII-ify apostrophes

        if not re.match("[-'a-zA-Z]*$", result): # non-ASCII word
            letters = list(result)

            for i, letter in enumerate(letters):
                if ord(letter) < 128: continue  # ASCII, nothing to do
                lower_letter = letter.lower()
                replacement = ''

                # vowels and semivowels
                if lower_letter in 'áàăâǎåäãąāả':
                    replacement = 'a'
                elif lower_letter in 'æ':
                    replacement = 'ae'
                elif lower_letter in 'éèĕêěëẽęēẻ':
                    replacement = 'e'
                elif lower_letter in 'íìĭîǐïĩįī':
                    replacement = 'i'
                elif lower_letter in 'óòŏôǒöõøǫōỏ':
                    replacement = 'o'
                elif lower_letter in 'œ':
                    replacement = 'oe'
                elif lower_letter in 'úùŭûǔůüũųūủ':
                    replacement = 'u'
                elif lower_letter in 'ýỳŷẙÿỹȳỷ':
                    replacement = 'y'
                # consonants
                elif lower_letter in 'ćĉčċç':
                    replacement = 'c'
                elif lower_letter in 'ďđ':
                    replacement = 'd'
                elif lower_letter in 'ǵğĝǧġḡ':
                    replacement = 'g'
                elif lower_letter in 'ĥȟḧħ':
                    replacement = 'h'
                elif lower_letter in 'ĵǰ':
                    replacement = 'j'
                elif lower_letter in 'ḱǩ':
                    replacement = 'k'
                elif lower_letter in 'ĺľłḹ':
                    replacement = 'l'
                elif lower_letter in 'ḿ':
                    replacement = 'm'
                elif lower_letter in 'ńǹňñ':
                    replacement = 'n'
                elif lower_letter in 'ṕ':
                    replacement = 'p'
                elif lower_letter in 'ŕř':
                    replacement = 'r'
                elif lower_letter in 'śŝšş':
                    replacement = 's'
                elif lower_letter in 'ß':
                    replacement = 'ss'
                elif lower_letter in 'ťẗ':
                    replacement = 't'
                elif lower_letter in 'ṽ':
                    replacement = 'v'
                elif lower_letter in 'ẃẁẘẅ':
                    replacement = 'w'
                elif lower_letter in 'ẍ':
                    replacement = 'x'
                elif lower_letter in 'źẑž':
                    replacement = 'z'

                if replacement:
                    if letter != lower_letter:  #  original was upper case
                        replacement = replacement.upper()
                    letters[i] = replacement

            result = ''.join(letters)
        return result

    def strip_genitive_s(self, word: str):
        """Strip the genitive or contraction "'s" from a word, if its present.

        Returns two values: actual word, genitive. If the word doesn't end in a genitive, the
        first return value will be identical to the argument and the second return value will
        be empty.

        Both simple and typographic apostrophes are recognized.
        """
        if re.search(".['’]s$", word, re.IGNORECASE):
            return word[:-2], word[-2:]
        else:
            return word, ''

    def translate_pos(self, spacy_pos: str) -> str:
        """Translate a POS tag as used by spaCy into the form used in the dictionary."""
        if spacy_pos in ('AUX', 'VERB'):
            return 'v'
        elif spacy_pos in ('NOUN', 'PROPN'):
            return 'n'
        elif spacy_pos == 'ADJ':
            return 'aj'
        elif spacy_pos == 'ADV':
            return 'av'
        elif spacy_pos == 'ADP':
            return 'prp'
        else:
            # Other POS tags shouldn't usually occur regarding our POS-tagged words,
            # but we use a reasonable default for cases where spaCy gets it wrong
            return 'v'

    def _find_pos_tagged_entry(self, entries: dict, pos: str,
            tried_variants: List[str] = None) -> str:
        """Find the entry to use if several POS-tagged spellings exist for a word.

        'pos' is the POS tag as returned by spaCy (capitalized) or used in our dictionary
        (lower-case) - the former will be translated into the later if needed.

        'tried_variants' is used internally to prevent endless loops in case another POS tag
        has to be tried as fallback.
        """
        if pos and pos[0].isupper():
            pos = self.translate_pos(pos)
        if tried_variants is None:
            tried_variants = []

        tried_variants.append(pos)
        result = entries.get(pos)

        if result is None:
            fallback = POS_FALLBACKS.get(pos)

            if fallback is None or fallback in tried_variants:
                if fallback is None:
                    warn("No fallback found for POS tag '{}'; using last entry of {} as fallback"
                            .format(pos, entries))
                else:
                    warn("Trying POS tag '{}' after {} would loop; using last entry of {} as "
                         "fallback".format(pos, tried_variants, entries))

                result = entries[sorted(entries.keys())[-1]]
            else:
                result = self._find_pos_tagged_entry(entries, fallback, tried_variants)

        return result


class Converter:
    """A converter from tradspell to Lytspell.

    Instances of this class are stateful and hence not threadsafe.
    """

    # Load Dictionary (once)
    _dict = Dictionary()

    # Load spaCy (once), disabling unnecessary components
    _nlp = spacy.load('en', disable=['parser', 'ner'])

    def __init__(self, use_unknown_counter: bool = False):
        """Create a new instance.

        Parameters:

        * use_unknown_counter: whether unknown words should be counted
        """
        # Switch consulted by self.lookup() and updated during conversion and tokenization
        self._at_sent_start = True
        # Optional counter of unknown words
        self._unknown_counter = Counter() if use_unknown_counter else None

    def lookup(self, word: str, spacy_pos: str = None) -> Union[str, ConvState]:
        """Lookup a word, delegating to the corrresponding 'Dictionary' method."""
        return self._dict.lookup(word, spacy_pos, self._at_sent_start)

    def tokenize_text(self, text: str) -> List[str]:
        """Tokenize a string, returning an array of words and puncuation.

        Words must start and end with a letter and may contain apostrophes.
        """
        if not text:
            return []  # Empty or None

        result = TOKEN_RE.split(text)
        # Remove first and/or last element if they are empty
        if result[0] == '':
            result.pop(0)
        if result[-1] == '':
            result.pop()
        return result

    def is_word(self, token: str) -> bool:
        """Check if a token is a word.

        Words start with a letter, or with an apostrophe followed by a letter."""
        return re.match("['’]?" + LOWERCASE_LETTER_PAT, token, re.IGNORECASE)

    def text_looks_foreign(self, text: str) -> bool:
        """Check whether a text fragment looks like it's written in a foreign language.

        Returns True iff a majority of the words are unknown.
        """
        in_tokens = self.tokenize_text(text)
        known_words = 0
        unknown_words = 0

        for token in in_tokens:
            if self.is_word(token):
                conv = self.lookup(token)
                if conv:
                    known_words += 1
                else:
                    unknown_words += 1

        return unknown_words > known_words

    def ends_sentence(self, token: str) -> bool:
        """Use some simple heuristics to determine whether a token seems to end a sentence.

        Returns True in one of three cases:

        * The tokens ends in a dot, question or exclamation mark, optionally followed by an
          opening or closing quote marks
        * It ends in a colon followed by an opening quote mark
        * It ends in '>' (quoted text marker), optionally followed by an opening quote mark

        Whitespace is ignored in all cases.
        """
        if not token:
            return False
        if re.search('([.?!](\s*["\'“‘”’])?|:\s*["\'“‘]|>(\s*["\'“‘])?)\s*$', token):
            return True
        else:
            return False

    def _update_at_sent_start(self, token: str) -> None:
        """Update the '_at_sent_start' attribute.

        Paramater:

        * token: the last read token which must be a non-word
        """
        local_value = self._at_sent_start
        if local_value:
            if re.search('\d', token):
                # Token contains digits, hence it is considered as first token of the new sentence
                local_value = False

        # Set to True if the token seems to end a sentence OR if the attribute was already True
        # (and not set to False by us due to encountering a number)
        self._at_sent_start = local_value or self.ends_sentence(token)

    def _convert_text_if_simple(self, text: str, test_if_foreign: bool = True,
            starts_sent: bool = True) -> Union[str, ConvState]:
        """Convert a text fragment if doing so is possible without POS tagging.

        'starts_sent' is an optional attribute that specifies whether this text fragment is
        likely to start with a new sentence. If true, the '_at_sent_start' switch is set
        accordingly, otherwise it is left alone.

        The return value is either:

        * The converted text (a string)
        * ConvState.NLP_NEEDED if POS tagging (NLP) is needed)
        * ConvState.LOOKS_FOREIGN if 'test_if_foreign' is true and a majority of the words in
          the fragment are unknown
        """
        orig_at_sent_start = self._at_sent_start  # Remember in case we have to restore it later

        if not text:
            return text  # Empty or None, nothing to do
        if starts_sent:
            self._at_sent_start = True

        in_tokens = self.tokenize_text(text)
        out_tokens = []
        lasttok = ''
        known_words = 0
        unknown_words = 0

        if self._unknown_counter is not None:
            local_unknown_counter = Counter()

        for token in in_tokens:
            if self.is_word(token):
                conv = self.lookup(token)

                if conv is ConvState.NLP_NEEDED:
                    self._at_sent_start = orig_at_sent_start
                    return ConvState.NLP_NEEDED
                elif conv:
                    known_words += 1
                    out_tokens.append(conv)
                else:
                    unknown_words += 1
                    out_tokens.append(token)

                    if self._unknown_counter is not None:
                        actualWord = self._dict.strip_genitive_s(token)[0]
                        if len(actualWord) > 1:
                            local_unknown_counter[actualWord] += 1

                self._at_sent_start = False
            else:
                # Not a word
                if token == '-' and lasttok:
                    # Check if this forms a hyphenated prefix with the preceding token, e.g. 're-'
                    conv = self.lookup(lasttok + token)

                    if conv:
                        out_tokens[-1] = conv
                    else:
                        out_tokens.append(token)
                else:
                    # Append non-word as is and check whether it terminates a sentence
                    out_tokens.append(token)
                    self._update_at_sent_start(token)

            lasttok = token

        if not test_if_foreign or unknown_words <= known_words:
            # We shouldn't make the foreign language test OR at least half of the words are known
            if self._unknown_counter is not None:
                self._unknown_counter += local_unknown_counter

            return ''.join(out_tokens)
        else:
            return ConvState.LOOKS_FOREIGN

    def convert_para(self, text: str, test_if_foreign: bool = True,
            starts_sent: bool = True) -> str:
        """Convert a paragraph.

        'starts_sent' is an optional attribute that specifies whether this text fragment is likely
        to start with a new sentence. If true, the 'self._at_sent_start' switch is set accordingly,
        otherwise it is left alone.

        If 'test_if_foreign' is True and a majority of the words in the paragraph are unknown,
        the paragraph is assumed to be written in a foreign language and returned unchanged.
        """
        if not text:
            if starts_sent:
                # Nothing to do, but we still update the global state (a new block-level HTML
                # element might have been opened)
                self._at_sent_start = True
            return text

        simple_result = self._convert_text_if_simple(text, test_if_foreign, starts_sent)

        if simple_result is ConvState.NLP_NEEDED:
            # We have to invoke spaCy for POS tagging (unless the text looks foreign)
            if self.text_looks_foreign(text):
                return text  # Return text unchanged

            doc = self._nlp(text)
            out_tokens = []
            lasttok = ''
            last_nonword = ''

            if starts_sent:
                self._at_sent_start = True

            for entry in doc:
                # Sometimes spaCy doesn't recognize all word boundaries, hence we run our own
                # tokenizer on each of its entries (unless it looks like an ULR or abbreviation)
                if entry.text.count('.') >= 2 or entry.text.count('/') >= 2:
                    in_tokens = [entry.text]
                    looks_like_url = True
                else:
                    in_tokens = self.tokenize_text(entry.text)
                    looks_like_url = False

                # Glue contraction tokens such as "'ll" back together
                if len(in_tokens) >= 2 and in_tokens[0] in ("'", "’") and \
                        self.is_word(in_tokens[1]):
                    in_tokens[0] += in_tokens[1]
                    in_tokens.pop(1)

                for idx, token in enumerate(in_tokens, start=1):
                    tail = ''

                    if idx == len(in_tokens) and entry.whitespace_:
                        tail = entry.whitespace_  # optional trailing whitespace after the entry

                    if token.lower() in ("n't", 'n’t'):
                        # SpaCy treats "n't" (as in "don't" etc.) as a separate word, but we look
                        # it up together with the preceding word because the joined pronounciation
                        # (and hence spelling) is sometimes different
                        if out_tokens:
                            out_tokens.pop()
                        token = lasttok + token

                        if self._unknown_counter is not None and lasttok in self._unknown_counter:
                            self._unknown_counter[lasttok] -= 1

                    if self.is_word(token):
                        conv = self.lookup(token, entry.pos_)

                        if conv:
                            out_tokens.append(conv)
                        else:
                            out_tokens.append(token)

                            if self._unknown_counter is not None and not looks_like_url:
                                actualWord = self._dict.strip_genitive_s(token)[0]
                                if len(actualWord) > 1:
                                    self._unknown_counter[actualWord] += 1

                        self._at_sent_start = self.ends_sentence(tail)
                        last_nonword = ''
                    else:
                        # Not a word
                        if token == '-' and not tail and lasttok:
                            # Check if this forms a hyphenated prefix with the preceding token,
                            # e.g. 're-'
                            conv = self.lookup(lasttok + token)

                            if conv:
                                out_tokens[-1] = conv
                            else:
                                out_tokens.append(token)
                                last_nonword = token
                        else:
                            # Append as is
                            out_tokens.append(token)

                            # We also consider the last nonword token for cases such as ‹: "›
                            # (colon followed by quote) which are considered two tokens by spaCy
                            self._update_at_sent_start(last_nonword + token + tail)
                            last_nonword = token

                    lasttok = token

                    # Append trailing whitespace to token, if any
                    if tail:
                        out_tokens[-1] += tail

            return ''.join(out_tokens)
        elif simple_result is ConvState.LOOKS_FOREIGN:
            return text  # Return text unchanged
        else:
            return simple_result

    def determine_file_type(self, filename: str) -> str:
        """Inspect the contents of a file to determine the likely file type.

        Returns either 'epub', 'html', 'txt', or None if the file is clearly not of any of
        these types. However, 'txt' is used as a fairly general fallback hence it's quite
        possible that a file labeled as 'txt' is actually something else
        """
        # Check if it's an epub file
        if is_zipfile(filename):
            with ZipFile(filename) as zip:
                bytes = b''

                try:
                    bytes = zip.read('mimetype')
                except KeyError:
                    pass  # Not an epub file

                if bytes.decode().startswith('application/epub+zip'):
                    return 'epub'
                else:
                    return None

        with open(filename) as file:
            for line in file:
                line = line.strip()
                if not line: continue  # Empty line, inspect next one
                if re.match("<[!?h]", line, re.IGNORECASE):
                    # File seems to start with a DOCTYPE or XML declaration, HTML comment or
                    # <html> tag
                    return 'html'
                else:
                    break

        return 'txt'

    def simple_tag(self, elem: etree._Element) -> str:
        """Return the tag name of an Element without the XHTML namespace, if used.

        If the element lives within the XHTML namespace, just the local name is returned,
        e.g. '{http://www.w3.org/1999/xhtml}img' becomes 'img'.

        In all other cases, the tag name is returned unchanged.
        """
        tag = elem.tag

        if tag is None:
            return tag

        if tag.startswith(XHTML_NAMESPACE):
            return tag[len(XHTML_NAMESPACE):]
        else:
            return tag

    def convert_html_elem(self, elem: etree._Element) -> None:
        """Recursively convert an element in an HTML document and its children.

        Whether to convert textual content is decided on the level of block-level tags (such
        as 'h1', 'blockquote', 'p') that do NOT contain any directly nested block-level tags
        (e.g. if an 'ol' contains 'li' elements, the decision will be made for each of the
        latter independently, not for the whole 'ol'). If a large part of the text embedded
        in such an element seems to be in a foreign language, the whole element will NOT be
        converted.
        """
        # Check if we should made the foreign-language test as this level
        if self.simple_tag(elem) in BLOCK_LEVEL_TAGS:
            decide_on_conversion = True

            for child in elem:
                if self.simple_tag(child) in BLOCK_LEVEL_TAGS:
                    decide_on_conversion = False
                    break

            if decide_on_conversion:
                full_text = str(etree.XPath("string()")(elem))
                if self.text_looks_foreign(full_text):
                    # Skip this part of the document tree (doesn't seem to be English)
                    return

        # Convert immediate content (only block-level elements count as start of a new paragraph)
        elem.text = self.convert_para(elem.text, False,
                starts_sent=self.simple_tag(elem) in BLOCK_LEVEL_TAGS)

        # Convert child elements (except comments and those that don't contain normal text)
        for child in elem:
            if not (isinstance(child, etree._Comment)
                    or isinstance(child, etree._ProcessingInstruction)
                    or self.simple_tag(child) in ('script', 'style')):
                self.convert_html_elem(child)

            if child.tail:
                child.tail = self.convert_para(child.tail, False, starts_sent=False)

        # Convert a few textual attributes, if they are present
        for attrib in ('alt', 'title'):
            if attrib in elem.attrib and elem.attrib[attrib]:
                elem.attrib[attrib] = self.convert_para(elem.attrib[attrib])

    def convert_html_document(self, filename_or_bytes: Union[str, BytesIO]) -> bytes:
        """Convert and return an HTML or XHTML file.

        Returns an UTF-8 encoded bytestring.
        """
        doc = None
        is_xhtml = True

        # Try parsing as XHTML, if that doesn't work, parse as regular HTML
        try:
            doc = etree.parse(filename_or_bytes)
        except etree.XMLSyntaxError:
            doc = html.parse(filename_or_bytes)
            is_xhtml = False

        root = doc.getroot()
        title = root.find('.//title', namespaces=root.nsmap)
        body = root.find('body', namespaces=root.nsmap)

        if title is not None:
            title.text = self.convert_para(title.text)
        if body is not None:
            self.convert_html_elem(body)

        if is_xhtml:
            return etree.tostring(doc, encoding='utf8')
        else:
            return html.tostring(doc, encoding='utf8')

    def convert_xml_elem(self, elem: etree._Element) -> None:
        """Recursively convert an element in an XML document and its children.

        This function is only meant for elements that aren't (X)HTML. All textual content of
        the element and its child elements are converted (unless they look foreign), while
        all attribute values are left alone. In HTML, on the other hand, certain elements
        (such as 'script') are skipped and certain attributes (such as 'alt') are converted.

        Also, while in HTML the foreign-language test is made at the level of block-level tags
        (such as 'p' or 'li'), here every text fragment is tested independently.
        """
        # Convert immediate content
        elem.text = self.convert_para(elem.text)

        # Convert child elements (except comments and PIs)
        for child in elem:
            if not (isinstance(child, etree._Comment)
                    or isinstance(child, etree._ProcessingInstruction)):
                self.convert_xml_elem(child)
            child.tail = self.convert_para(child.tail, starts_sent=False)

    def convert_xml_document(self, filename_or_bytes: Union[str, BytesIO]) -> bytes:
        """Convert and return an XML file.

        This function is only meant for documents that aren't (X)HTML.

        Returns an UTF-8 encoded bytestring.
        """
        doc = etree.parse(filename_or_bytes)
        self.convert_xml_elem(doc.getroot())
        return etree.tostring(doc, encoding='utf8')

    def _make_hrefs_absolute(self, items: Sequence[etree._Element],
            dirname: str) -> Sequence[etree._Element]:
        """Convert a list of XML elements with filenames from relative into absolute filenames.

        Each member of the 'items' sequence must have a 'href' attribute that will be
        modified accordingly.

        If dirname is empty, the original list is returned unchanged.
        """
        if dirname:
            for item in items:
                item.attrib['href'] = path.join(dirname, item.attrib['href'])

        return items

    def convert_epub(self, filename: str) -> None:
        """Convert an epub file.

        If the input file is called 'FILE.epub', the output will be stored in a new file called
        'FILE-lytspel.epub'.
        """
        (root, ext) = path.splitext(filename)
        outfile = '{}-lytspel{}'.format(root, ext)

        with ZipFile(filename, 'r') as zin:
            # Find the contents metafile
            bytes = zin.read('META-INF/container.xml')
            tree = etree.fromstring(bytes)
            # Usually there is just one OPF file, but multiple-rendition epubs have several ones
            opf_files = tree.xpath('n:rootfiles/n:rootfile/@full-path',
                    namespaces={'n': 'urn:oasis:names:tc:opendocument:xmlns:container'})
            absolute_items = []

            # Find the XML files that need conversion
            for opf_file in opf_files:
                bytes = zin.read(opf_file)
                tree = etree.fromstring(bytes)
                relative_items = tree.xpath('/p:package/p:manifest/p:item',
                        namespaces={'p': 'http://www.idpf.org/2007/opf'})
                absolute_items += self._make_hrefs_absolute(relative_items,
                        path.dirname(opf_file))

            html_files = set()
            ncx_files = set()

            for item in absolute_items:
                media_type = item.attrib['media-type']

                if media_type == 'application/xhtml+xml':
                    html_files.add(item.attrib['href'])
                elif media_type == 'application/x-dtbncx+xml':
                    # Deprecated, but might occur in older epubs
                    ncx_files.add(item.attrib['href'])

            # Copy files to output archive, converting them if needed
            with ZipFile(outfile, 'w') as zout:
                zout.comment = zin.comment  # Preserve the comment, if any

                for item in zin.infolist():
                    bytes = zin.read(item.filename)

                    if item.filename in html_files:
                        bio = BytesIO(bytes)
                        bytes = self.convert_html_document(bio)
                    if item.filename in ncx_files or item.filename in opf_files:
                        bio = BytesIO(bytes)
                        bytes = self.convert_xml_document(bio)

                    zout.writestr(item, bytes)

        print('{}: Output written to {}'.format(SCRIPTNAME, outfile), file=stderr)

    def convert_text_document(self, filename: str) -> None:
        """Convert a plain text file."""
        para = ''

        with open(filename) as file:
            for line in file:
                line = line.rstrip()
                # Paragraphs are considered to be separated by empty lines. However, very long
                # lines (200+ chars) are considered paragraphs in their own right.
                if len(line) >= 200:  # stand-alone paragraph
                    if para:
                        print(self.convert_para(para))
                        para = ''
                    print(self.convert_para(line))
                elif line:            # regular line
                    if para:
                        para += '\n'
                    para += line
                else:                 # empty line
                    if para:
                        print(self.convert_para(para))
                        para = ''
                    print()

            # Convert final paragraph, if any
            if para:
                print(self.convert_para(para))

    def convert_file(self, filename: str) -> None:
        """Convert the file with the specified name.

        Recognized file types are epub, HTML and plain text.
        """
        file_type = self.determine_file_type(file)

        if path.getsize(filename) / 1024 >= 256:
            print('{}: Converting {} -- this may take a while...'.format(SCRIPTNAME, filename),
                    file=stderr)

        if file_type == 'epub':
            self.convert_epub(file)
        elif file_type == 'html':
            bytes = self.convert_html_document(file)
            print(bytes.decode('utf8'))
        elif file_type == 'txt':
            self.convert_text_document(file)
        elif file_type is None:
            exit('{}: Cannot convert {} (unsupported file type)'.format(SCRIPTNAME, filename))
        else:
            raise ValueError('Unexpected file type: {}'.format(file_type))

    def _unify_case_differences(self) -> None:
        """Unify entries within the internal unknown word counter that differ only by case.

        Calling this method when this instance had been initialized with 'use_unknown_counter' =
        False will trigger an error.

        Any counts of capitalized words to the count of the lower-case variant if it exists,
        deleting the alternative entries. For example, Counter(hulla=3, Hulla=1, HULLA=1)
        becomes Counter(hulla=5).

        If there is no lower-case variant, other variants that differ in case will NOT be
        unified. For example, Counter(Hulla=1, HULLA=1) remains unchanged.
        """
        entries_to_delete = []

        for key, count in self._unknown_counter.items():
            if any(x.isupper() for x in key):
                lower = key.lower()

                if lower in self._unknown_counter:
                   self._unknown_counter[lower] += count
                   entries_to_delete.append(key)

        for entry in entries_to_delete:
            del self._unknown_counter[entry]

    def print_unknown_words(self, min_count: int = 1) -> None:
        """Writes the list of unknown words encountered in the converted texts to stderr.

        Only words encountered at least 'min_count' times are shown.

        Calling this method when this instance had been initialized with 'use_unknown_counter' =
        False will trigger a warning.
        """
        if self._unknown_counter is None:
            warn('print_unknown_words called on a Converter without an unknown_counter')
            return

        header_shown = False
        self._unify_case_differences()

        for key, count in sorted(self._unknown_counter.items(), key=lambda pair: pair[0].lower()):
            if count < min_count:
                continue

            output = '  ' + key

            if count > 1:
                output += ' ({}x)'.format(count)
            if not header_shown:
                print('Unknown words:', file=stderr)
                header_shown = True

            print(output, file=stderr)

        if not header_shown:
            if min_count <= 1:
                print('No unknown words', file=stderr)
            else:
                print('No unknown words (occurring {} times or more)'.format(min_count),
                        file=stderr)


if __name__ == '__main__':
    try:
        parser = argparse.ArgumentParser()
        parser.add_argument('files', metavar='FILE', nargs='*',
                help='files to convert')
        parser.add_argument('-u', '--unknown', action='count', default=0,
                help='list unknown words (repeat option n times to list only words that occur'
                    ' at least that often)')
        args = parser.parse_args()

        use_unknown_counter = args.unknown > 0
        conv = Converter(use_unknown_counter)

        for file in args.files:
            conv.convert_file(file)

        if use_unknown_counter:
            conv.print_unknown_words(args.unknown)
    except Exception as e:
        tb = exc_info()[2]
        # Find highest stack frame in the current file (closest to source of exception)
        frames = trace()
        last_useful_frame = frames[0]
        current_file = last_useful_frame.filename

        for frame in frames[1:]:
            if frame.filename == current_file:
                last_useful_frame = frame
            else:
                break  # Stepping out of current file

        fname = path.split(current_file)[1]
        exit('{}:{}: {}: {}'.format(fname, last_useful_frame.lineno, e.__class__.__name__, e))


# TODO Handle and test:
# * Allow converting text passed on command line (-c flag)
# * Epub conversion: ensure that zip compression is as it should be
# * Use pylint to check the coding style
# * -o flag to specify an output file (requires a single input file)
# * Accept '-' argument to read stdin (file type recognition should still work)
# * HTML conversion: check whether complex mixed text handled correctly (foreign language testing)
# * Profile epub conversion and remove any obvious bottlenecks
# * Package for pip3 and announce to the world
